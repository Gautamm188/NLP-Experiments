{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2aFZe_5n3fE",
        "outputId": "b9c93fb3-552b-4a87-8d38-379da7ed845f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "example sentence demonstrating removal stopwords.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "text = \"This is an example sentence demonstrating the removal of stopwords.\"\n",
        "text_without_stopwords = remove_stopwords(text)\n",
        "print(text_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we first import the stopwords corpus from NLTK. Then, we define a function remove_stopwords that takes a piece of text as input. Inside the function, we retrieve the set of English stopwords using stopwords.words(\"english\"). We then split the input text into words, filter out the stopwords, and finally join the remaining words back into a single string. The resulting string is the input text with stopwords removed."
      ],
      "metadata": {
        "id": "Hg7J8uuTn_Lf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    # Removing stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [token for token in stemmed_tokens if token.lower() not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "\n",
        "    return lemmatized_tokens\n",
        "\n",
        "sentence = \"This is an example sentence demonstrating the removal of stopwords.\"\n",
        "processed_tokens = preprocess_text(sentence)\n",
        "print(processed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7etYqAkqsm7",
        "outputId": "cca887ab-0d70-46fc-bf72-5ca2acc02f5d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['thi', 'exampl', 'sentenc', 'demonstr', 'remov', 'stopword', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ee4iLyRVqxhL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}