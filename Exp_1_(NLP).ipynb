{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. Install NLTK and download necessary resources:**"
      ],
      "metadata": {
        "id": "blQbyeGKhRF1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIlF3syYhPG4",
        "outputId": "15a7c3d5-9048-4eb9-fdd7-64d3a2ee5b84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')  # For sentence segmentation\n",
        "nltk.download('averaged_perceptron_tagger')  # For part-of-speech tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Prepare your text:**"
      ],
      "metadata": {
        "id": "NlbW17fWhaav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This Gautam's is a sample text to demonstrate different tokenization techniques in NLTK. It includes punctuation, numbers, and abbreviations.\""
      ],
      "metadata": {
        "id": "fnPpszBGhYib"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spliting"
      ],
      "metadata": {
        "id": "4HJECnzyfQzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the sentence using the split() method:\n",
        "split_tokens = text.split()\n",
        "print(\"Split Tokens:\", split_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeU7Qix8fU-L",
        "outputId": "ee987443-bc94-4669-a59e-d70705b1f77e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split Tokens: ['This', \"Gautam's\", 'is', 'a', 'sample', 'text', 'to', 'demonstrate', 'different', 'tokenization', 'techniques', 'in', 'NLTK.', 'It', 'includes', 'punctuation,', 'numbers,', 'and', 'abbreviations.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Apply different tokenization techniques:**"
      ],
      "metadata": {
        "id": "UpGVSj8Shrax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a. Word Tokenization:"
      ],
      "metadata": {
        "id": "uwPjkiFhh2ZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "word_tokens = word_tokenize(text)\n",
        "print(\"Word Tokens:\", word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyhoLe4chkSR",
        "outputId": "5a47186a-ea8c-4069-fa76-d972cecabc4b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokens: ['This', 'Gautam', \"'s\", 'is', 'a', 'sample', 'text', 'to', 'demonstrate', 'different', 'tokenization', 'techniques', 'in', 'NLTK', '.', 'It', 'includes', 'punctuation', ',', 'numbers', ',', 'and', 'abbreviations', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. Sentence Tokenization:"
      ],
      "metadata": {
        "id": "SLdjiwMNh-MF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sentence_tokens = sent_tokenize(text)\n",
        "print(\"Sentence Tokens:\", sentence_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbFr05Ejh7KG",
        "outputId": "6b029052-6225-4367-ae86-ec8154db0942"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokens: [\"This Gautam's is a sample text to demonstrate different tokenization techniques in NLTK.\", 'It includes punctuation, numbers, and abbreviations.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "c. TreebankWordTokenizer (handles contractions and hyphens):"
      ],
      "metadata": {
        "id": "sawHS_ojiCbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "treebank_tokenizer = TreebankWordTokenizer()\n",
        "treebank_tokens = treebank_tokenizer.tokenize(text)\n",
        "print(\"Treebank Tokens:\", treebank_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToMWrjTUiAdv",
        "outputId": "0e84c2f6-650a-4594-f5e2-a0526932aa29"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treebank Tokens: ['This', 'Gautam', \"'s\", 'is', 'a', 'sample', 'text', 'to', 'demonstrate', 'different', 'tokenization', 'techniques', 'in', 'NLTK.', 'It', 'includes', 'punctuation', ',', 'numbers', ',', 'and', 'abbreviations', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "d. RegexpTokenizer (customizable with regular expressions):"
      ],
      "metadata": {
        "id": "wfS4JOzmiIhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "regexp_tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+')  # Split on words, dollar amounts, or any non-whitespace characters\n",
        "regexp_tokens = regexp_tokenizer.tokenize(text)\n",
        "print(\"Regexp Tokens:\", regexp_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1_8CXD0iEcD",
        "outputId": "f79d29df-d442-4be9-a559-934134bde8ea"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regexp Tokens: ['This', 'Gautam', \"'s\", 'is', 'a', 'sample', 'text', 'to', 'demonstrate', 'different', 'tokenization', 'techniques', 'in', 'NLTK', '.', 'It', 'includes', 'punctuation', ',', 'numbers', ',', 'and', 'abbreviations', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "e. TweetTokenizer (optimized for tweets):"
      ],
      "metadata": {
        "id": "tXmdWmE9iNDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "tweet_tokens = tweet_tokenizer.tokenize(text)\n",
        "print(\"Tweet Tokens:\", tweet_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Sq3wMWNiKq4",
        "outputId": "417b5638-6a80-4fe4-c037-ab432468e0e7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet Tokens: ['This', \"Gautam's\", 'is', 'a', 'sample', 'text', 'to', 'demonstrate', 'different', 'tokenization', 'techniques', 'in', 'NLTK', '.', 'It', 'includes', 'punctuation', ',', 'numbers', ',', 'and', 'abbreviations', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "# Prepare the text\n",
        "text = \"This is Gautam's sample text. It includes punctuation, numbers, and abbreviations.\"\n",
        "# Create a PunktSentenceTokenizer object\n",
        "custom_tokenizer = PunktSentenceTokenizer()\n",
        "# Tokenize the text\n",
        "custom_tokens = custom_tokenizer.tokenize(text)\n",
        "# Print the tokens\n",
        "print(\"Custom Punkt Tokens:\", custom_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoKYTRP3fbAJ",
        "outputId": "fd9b843a-6682-4df9-d87e-2059f9531c59"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Punkt Tokens: [\"This is Gautam's sample text.\", 'It includes punctuation, numbers, and abbreviations.']\n"
          ]
        }
      ]
    }
  ]
}