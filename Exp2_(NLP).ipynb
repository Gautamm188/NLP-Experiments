{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Stemming and lemmatization are both techniques used in natural language processing to reduce words to their base or root forms. While stemming usually involves chopping off suffixes to obtain the word stem, lemmatization involves reducing words to their dictionary or lemma form. Here's how you can implement both concepts using Python:**"
      ],
      "metadata": {
        "id": "6fp8YvyTjo1f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Stemming with NLTK:**\n",
        "NLTK (Natural Language Toolkit) is a popular library for text processing in Python. It provides various stemming algorithms, including Porter and Lancaster stemmers."
      ],
      "metadata": {
        "id": "HxsKLC-kjdW4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaKd35tkiGHq",
        "outputId": "9eef3d1d-4fac-4bc6-c2d1-463ef017934b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stem is the process of reduc word to their base or root form\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def stem_text(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_words = [stemmer.stem(word) for word in text.split()]\n",
        "    return ' '.join(stemmed_words)\n",
        "\n",
        "text = \"Stemming is the process of reducing words to their base or root forms\"\n",
        "stemmed_text = stem_text(text)\n",
        "print(stemmed_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Porter Stemmer:**\n",
        "The Porter stemming algorithm is one of the most commonly used stemming algorithms. It's available in NLTK."
      ],
      "metadata": {
        "id": "wXKyqlhqmsOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def porter_stem(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_words = [stemmer.stem(word) for word in text.split()]\n",
        "    return ' '.join(stemmed_words)\n",
        "\n",
        "text = \"Stemming is the process of reducing words to their base or root forms\"\n",
        "stemmed_text_porter = porter_stem(text)\n",
        "print(stemmed_text_porter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsL_TDQQmqu2",
        "outputId": "e347b7c2-85c6-42e2-b983-3ff51e19d097"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stem is the process of reduc word to their base or root form\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Snowball Stemmer (Porter2 or English Stemmer):**\n",
        "The Snowball stemmer is a more modern and slightly improved version of the Porter stemmer."
      ],
      "metadata": {
        "id": "KRYbM335m5Kf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "def snowball_stem(text):\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    stemmed_words = [stemmer.stem(word) for word in text.split()]\n",
        "    return ' '.join(stemmed_words)\n",
        "\n",
        "text = \"Stemming is the process of reducing words to their base or root forms\"\n",
        "stemmed_text_snowball = snowball_stem(text)\n",
        "print(stemmed_text_snowball)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkyLzHPGm8m2",
        "outputId": "2b92b467-aa96-45ee-f2fd-4c474c6f21ae"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stem is the process of reduc word to their base or root form\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lancaster Stemmer:**\n",
        "The Lancaster stemmer is a more aggressive stemming algorithm compared to the Porter stemmer."
      ],
      "metadata": {
        "id": "1MIZ-f9YnAE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "def lancaster_stem(text):\n",
        "    stemmer = LancasterStemmer()\n",
        "    stemmed_words = [stemmer.stem(word) for word in text.split()]\n",
        "    return ' '.join(stemmed_words)\n",
        "\n",
        "text = \"Stemming is the process of reducing words to their base or root forms\"\n",
        "stemmed_text_lancaster = lancaster_stem(text)\n",
        "print(stemmed_text_lancaster)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdxODXD_nHFW",
        "outputId": "2bd16db1-6cf3-419e-81ee-01bf38cd79ed"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stem is the process of reduc word to their bas or root form\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Lemmatization with NLTK:**\n",
        "NLTK also provides lemmatization capabilities using WordNet."
      ],
      "metadata": {
        "id": "nJ2Opr4YjjlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhyH_G-biNeS",
        "outputId": "74b1291b-f55d-436f-f96a-3c5e572dc7a4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in text.split()]\n",
        "    return ' '.join(lemmatized_words)\n",
        "\n",
        "text = \"Lemmatization reduces words to their dictionary or lemma form\"\n",
        "lemmatized_text = lemmatize_text(text)\n",
        "print(lemmatized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaExussWiSxX",
        "outputId": "c34a47b6-5066-4cc1-c26a-c25784ee6b9f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatization reduces word to their dictionary or lemma form\n"
          ]
        }
      ]
    }
  ]
}